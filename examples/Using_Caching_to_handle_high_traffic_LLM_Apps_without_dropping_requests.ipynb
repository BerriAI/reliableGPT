{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zCmtRC5gx6YM"
      },
      "source": [
        "# Using Caching to handle high-traffic LLM Apps without dropping requests\n",
        "\n",
        "ðŸš€ **Do this in 1 line of code [reliableGPT](https://github.com/BerriAI/reliableGPT)**\n",
        "\n",
        "```reliableGPT(openai.ChatCompletion.create, caching=True)```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EDiaaoOCx3DP"
      },
      "source": [
        "## Environment Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fINQBJ720l_g"
      },
      "outputs": [],
      "source": [
        "!pip install openai waitress chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jq-Ja3Ds1PNe"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = \"YOUR_OPENAI_KEY\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8DF9RAqUzp79"
      },
      "source": [
        "## Create our Server\n",
        "\n",
        "We'll probably be making OpenAI calls from our backend server. Let's do that now!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bg_brJ21x485"
      },
      "source": [
        "### Wrap OpenAI\n",
        "\n",
        "We'll wrap around the main openai chatcompletions endpoint, this will ensure we're able to cache our OpenAI responses, and reply with them later."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key components:\n",
        "### - `chromadb`: We're using this as our in-memory cache.\n",
        "### - `wrapper_fn`: We're wrapping the OpenAI chat endpoint with this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "yqx_TbbyzAVo"
      },
      "outputs": [],
      "source": [
        "from threading import active_count # get the number of active threads being used\n",
        "import chromadb\n",
        "from uuid import uuid4\n",
        "import traceback\n",
        "\n",
        "# In-memory Cache!\n",
        "chroma_client = chromadb.Client()\n",
        "cache = chroma_client.create_collection(name=\"cache\")\n",
        "\n",
        "# Our Wrapper Function!!\n",
        "def wrapper_fn(fn, max_threads=1):\n",
        "  def wrapped_fn(*args, **kwargs):\n",
        "    try:\n",
        "      thread_utilization = active_count()/max_threads\n",
        "      if thread_utilization > 0.85: # if thread utilization is > 85% (high load!!), let's respond with cached requests\n",
        "        if \"messages\" in kwargs:\n",
        "          try:\n",
        "            if cache.count() > 0:\n",
        "              input_prompt = \"\\n\".join(message[\"content\"]\n",
        "                                        for message in kwargs[\"messages\"])\n",
        "              result = cache.query(\n",
        "                query_texts=[input_prompt],\n",
        "                n_results=1\n",
        "              )\n",
        "              return result\n",
        "            else:\n",
        "              pass\n",
        "          except:\n",
        "            pass\n",
        "      result = fn(*args, **kwargs)\n",
        "      if \"messages\" in kwargs:\n",
        "        input_prompt = \"\\n\".join(message[\"content\"]\n",
        "                                    for message in kwargs[\"messages\"])\n",
        "        cache.add(\n",
        "          documents=[input_prompt],\n",
        "          metadatas=[{\"result\": result.choices[0].message.content}],\n",
        "          ids=[str(uuid4())],\n",
        "        )\n",
        "      return result.choices[0].message.content\n",
        "    except:\n",
        "      traceback.print_exc()\n",
        "  return wrapped_fn"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key components:\n",
        "### - `openai.ChatCompletion.create`: We're wrapping this endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQwn8FI90H0h",
        "outputId": "ea7bea8c-0b7b-45a4-ff94-1a66453a3bf3"
      },
      "outputs": [],
      "source": [
        "# Wrapping the OpenAI endpoint!\n",
        "original_fn = openai.ChatCompletion.create # let's keep the original reference in case we need it\n",
        "openai.ChatCompletion.create = wrapper_fn(openai.ChatCompletion.create)\n",
        "\n",
        "# Test OpenAI Call\n",
        "chat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
        "print(chat_completion)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cN1EeY1BzA-r"
      },
      "source": [
        "### Create our Flask Server\n",
        "\n",
        "Let's make our flask server - we'll also use the waitress package to handle queuing, threading, etc. for our server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "L7B3WkuVzTir"
      },
      "outputs": [],
      "source": [
        "# Creating our Flask Server!!\n",
        "import threading\n",
        "from flask import Flask, request\n",
        "from waitress import serve\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/test_func\")\n",
        "def test_fn():\n",
        "  result = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
        "  return result\n",
        "\n",
        "# Method to run our flask app (need to do this, since we're running this in a Jupyter Notebook)\n",
        "def run_app():\n",
        "  from waitress import serve\n",
        "  serve(app, host='0.0.0.0', port=3000, threads=1)\n",
        "\n",
        "# Run our flask app\n",
        "# Starting a Python thread that will run the flask application\n",
        "flask_thread = threading.Thread(target=run_app)\n",
        "flask_thread.start()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "St4nNfPgzT4n"
      },
      "source": [
        "# ðŸš€ Test with traffic!\n",
        "\n",
        "Let's send 100+ simultaneous calls to openai and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGYZx8xR0PlP",
        "outputId": "4d8ac890-537a-4d38-92b2-3f7f1b621049"
      },
      "outputs": [],
      "source": [
        "# Making 100+ calls at once!\n",
        "import concurrent.futures\n",
        "\n",
        "# Function to call test_fn()\n",
        "def call_test_fn():\n",
        "  with app.test_client() as client:\n",
        "      response = client.get('/test_func')\n",
        "      return response.data\n",
        "\n",
        "# Call the function to test the Flask endpoint\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=800) as executor:\n",
        "  # Submit the requests and gather the future objects\n",
        "  futures = [executor.submit(call_test_fn) for _ in range(100)]\n",
        "\n",
        "  # Wait for all futures to complete\n",
        "  concurrent.futures.wait(futures)\n",
        "\n",
        "  # Retrieve the results\n",
        "  results = [future.result() for future in futures]\n",
        "\n",
        "for result in results:\n",
        "  print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

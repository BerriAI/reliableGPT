{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Build custom model fallback strategy for Langchain Models\n",
        "\n",
        "\n",
        "\n",
        "**ðŸš€ Do this in 1 line of code with [reliableGPT](https://github.com/BerriAI/reliableGPT)**\n",
        "\n",
        "`reliableGPT(openai.ChatCompletion.create)`\n",
        "\n",
        "ðŸ‘‰ [Replit](https://replit.com/@krrishdholakia/CustomModelFallback?v=1)\n",
        "\n",
        "Contributions welcome ðŸ˜Š"
      ],
      "metadata": {
        "id": "zyTwr0wDm-tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Set-Up"
      ],
      "metadata": {
        "id": "7_OqTBsYTkce"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNUqxG4KTgu_"
      },
      "outputs": [],
      "source": [
        "!pip install openai langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai, os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "chat.predict(\"Translate this sentence from English to French. I love programming.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DsIA5QxMU9z3",
        "outputId": "4a05c39e-fb19-49ff-f472-f8ffb7258e4f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"J'adore la programmation.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing model fallback strategy\n",
        "\n",
        "Here's how we'll do this:\n",
        "- `Wrapper Function`: We'll wrap the OpenAI endpoint, to catch any errors that might occur\n",
        "- `Fallback Strategy`: List of models we want to rotate through, in case one fails"
      ],
      "metadata": {
        "id": "D-3pXRrWTmmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import traceback\n",
        "import copy\n",
        "import random\n",
        "\n",
        "def fallback_fn(fn, args, kwargs, fallback_strategy):\n",
        "  try:\n",
        "    for model in fallback_strategy:\n",
        "      kwargs['model'] = model\n",
        "      completion = fn(**kwargs)\n",
        "      if completion != None:\n",
        "        break\n",
        "    return completion\n",
        "  except:\n",
        "    traceback.print_exc()\n",
        "    return None\n",
        "\n",
        "def throw_random_error():\n",
        "  if random.random() <= 0.2:\n",
        "      raise Exception(\"Random error occurred!\")\n",
        "  else:\n",
        "      print(\"No error occurred.\")\n",
        "\n",
        "# Our Wrapper Function!!\n",
        "def wrapper_fn(fn, fallback_strategy=[\"gpt-4\", \"gpt-3.5-turbo-16k\", \"gpt-3.5-turbo\"]): # setting our default fallback strategy, in case OpenAI fails\n",
        "  def wrapped_fn(*args, **kwargs):\n",
        "    try:\n",
        "      print(\"received request!\")\n",
        "      throw_random_error() # let's simulate an OpenAI error\n",
        "      result = fn(*args, **kwargs)\n",
        "    except Exception as e:\n",
        "      traceback.print_exc()\n",
        "      result = fallback_fn(fn, args, kwargs, fallback_strategy=fallback_strategy)\n",
        "      if result == None:\n",
        "        raise e\n",
        "      else:\n",
        "        print(f\"ðŸš€Recovered result: {result}\")\n",
        "    return result\n",
        "  return wrapped_fn"
      ],
      "metadata": {
        "id": "lYCyopMVUV4S"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapping our OpenAI call!"
      ],
      "metadata": {
        "id": "ACBYPjhW0gln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_fn = openai.ChatCompletion.create # save the original reference (good practice)\n",
        "openai.ChatCompletion.create = wrapper_fn(openai.ChatCompletion.create)"
      ],
      "metadata": {
        "id": "iIabJS530jls"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš€ Test with 100 OpenAI Calls"
      ],
      "metadata": {
        "id": "TtMoyw5V0sXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(100):\n",
        "  chat_completion = chat.predict(\"Hello world!\")\n",
        "  print(chat_completion)"
      ],
      "metadata": {
        "id": "BKXk-GuG0y5d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}